{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Programming assignment 3: Optimization - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Your task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this notebook code skeleton for performing logistic regression with gradient descent is given. \n",
    "Your task is to complete the functions where required. \n",
    "You are only allowed to use built-in Python functions, as well as any `numpy` functions. No other libraries / imports are allowed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For numerical reasons, we actually minimize the following loss function\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w}) = \\frac{1}{N} NLL(\\mathbf{w}) +  \\frac{1}{2}\\lambda ||\\mathbf{w}||^2_2$$\n",
    "\n",
    "where $NLL(\\mathbf{w})$ is the negative log-likelihood function, as defined in the lecture (see  Eq. 33)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Exporting the results to PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Once you complete the assignments, export the entire notebook as PDF and attach it to your homework solutions. \n",
    "The best way of doing that is\n",
    "1. Run all the cells of the notebook.\n",
    "2. Export/download the notebook as PDF (File -> Download as -> PDF via LaTeX (.pdf)).\n",
    "3. Concatenate your solutions for other tasks with the output of Step 2. On a Linux machine you can simply use `pdfunite`, there are similar tools for other platforms too. You can only upload a single PDF file to Moodle.\n",
    "\n",
    "Make sure you are using `nbconvert` Version 5.5 or later by running `jupyter nbconvert --version`. Older versions clip lines that exceed page width, which makes your code harder to grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Load and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this assignment we will work with the UCI ML Breast Cancer Wisconsin (Diagnostic) dataset https://goo.gl/U2Uwz2.\n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. There are 212 malignant examples and 357 benign examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Add a vector of ones to the data matrix to absorb the bias term\n",
    "X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "\n",
    "# Set the random seed so that we have reproducible experiments\n",
    "np.random.seed(123)\n",
    "\n",
    "# Split into train and test\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Task 1: Implement the sigmoid function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"\n",
    "    Applies the sigmoid function elementwise to the input data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : array, arbitrary shape\n",
    "        Input data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    t_sigmoid : array, arbitrary shape.\n",
    "        Data after applying the sigmoid function.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Task 2: Implement the negative log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As defined in Eq. 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def negative_log_likelihood(X, y, w):\n",
    "    \"\"\"\n",
    "    Negative Log Likelihood of the Logistic Regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Classification targets.\n",
    "    w : array, shape [D]\n",
    "        Regression coefficients (w[0] is the bias term).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    nll : float\n",
    "        The negative log likelihood.\n",
    "    \"\"\"\n",
    "    scores = sigmoid(np.dot(X, w))\n",
    "    nll = -np.sum(y*np.log(scores+1e-15) + (1-y)*np.log(1-scores+1e-15))\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Computing the loss function $\\mathcal{L}(\\mathbf{w})$ (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def compute_loss(X, y, w, lmbda):\n",
    "    \"\"\"\n",
    "    Negative Log Likelihood of the Logistic Regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Classification targets.\n",
    "    w : array, shape [D]\n",
    "        Regression coefficients (w[0] is the bias term).\n",
    "    lmbda : float\n",
    "        L2 regularization strength.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        Loss of the regularized logistic regression model.\n",
    "    \"\"\"\n",
    "    # The bias term w[0] is not regularized by convention\n",
    "    return negative_log_likelihood(X, y, w) / len(y) + lmbda * 0.5 * np.linalg.norm(w[1:])**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Task 3: Implement the gradient $\\nabla_{\\mathbf{w}}\\mathcal{L}(\\mathbf{w})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Make sure that you compute the gradient of the loss function $\\mathcal{L}(\\mathbf{w})$ (not simply the NLL!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_gradient(X, y, w, mini_batch_indices, lmbda):\n",
    "    \"\"\"\n",
    "    Calculates the gradient (full or mini-batch) of the negative log likelilhood w.r.t. w.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Classification targets.\n",
    "    w : array, shape [D]\n",
    "        Regression coefficients (w[0] is the bias term).\n",
    "    mini_batch_indices: array, shape [mini_batch_size]\n",
    "        The indices of the data points to be included in the (stochastic) calculation of the gradient.\n",
    "        This includes the full batch gradient as well, if mini_batch_indices = np.arange(n_train).\n",
    "    lmbda: float\n",
    "        Regularization strentgh. lmbda = 0 means having no regularization.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dw : array, shape [D]\n",
    "        Gradient w.r.t. w.\n",
    "    \"\"\"\n",
    "    n_batch = mini_batch_indices.shape[0]\n",
    "    \n",
    "    nll_gradient = np.dot(X[mini_batch_indices].T,sigmoid(np.dot(X[mini_batch_indices], w)) - y[mini_batch_indices])\n",
    "    \n",
    "    ones = np.ones(w.shape)\n",
    "    ones[0] = 0\n",
    "    reg_gradient = lmbda * ones * w\n",
    "    \n",
    "    # we could also do (nll_gradient + reg_gradient) / n_batch.\n",
    "    # this will only change how we need to tune lambda in practice\n",
    "    grad = nll_gradient / n_batch + reg_gradient  \n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Train the logistic regression model (nothing to do here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, num_steps, learning_rate, mini_batch_size, lmbda, verbose):\n",
    "    \"\"\"\n",
    "    Performs logistic regression with (stochastic) gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Classification targets.\n",
    "    num_steps : int\n",
    "        Number of steps of gradient descent to perform.\n",
    "    learning_rate: float\n",
    "        The learning rate to use when updating the parameters w.\n",
    "    mini_batch_size: int\n",
    "        The number of examples in each mini-batch.\n",
    "        If mini_batch_size=n_train we perform full batch gradient descent. \n",
    "    lmbda: float\n",
    "        Regularization strentgh. lmbda = 0 means having no regularization.\n",
    "    verbose : bool\n",
    "        Whether to print the loss during optimization.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape [D]\n",
    "        Optimal regression coefficients (w[0] is the bias term).\n",
    "    trace: list\n",
    "        Trace of the loss function after each step of gradient descent.\n",
    "    \"\"\"\n",
    "    \n",
    "    trace = [] # saves the value of loss every 50 iterations to be able to plot it later\n",
    "    n_train = X.shape[0] # number of training instances\n",
    "    \n",
    "    w = np.zeros(X.shape[1]) # initialize the parameters to zeros\n",
    "    \n",
    "    # run gradient descent for a given number of steps\n",
    "    for step in range(num_steps):\n",
    "        permuted_idx = np.random.permutation(n_train) # shuffle the data\n",
    "        \n",
    "        # go over each mini-batch and update the paramters\n",
    "        # if mini_batch_size = n_train we perform full batch GD and this loop runs only once\n",
    "        for idx in range(0, n_train, mini_batch_size):\n",
    "            # get the random indices to be included in the mini batch\n",
    "            mini_batch_indices = permuted_idx[idx:idx+mini_batch_size]\n",
    "            gradient = get_gradient(X, y, w, mini_batch_indices, lmbda)\n",
    "\n",
    "            # update the parameters\n",
    "            w = w - learning_rate * gradient\n",
    "        \n",
    "        # calculate and save the current loss value every 50 iterations\n",
    "        if step % 50 == 0:\n",
    "            loss = compute_loss(X, y, w, lmbda)\n",
    "            trace.append(loss)\n",
    "            # print loss to monitor the progress\n",
    "            if verbose:\n",
    "                print('Step {0}, loss = {1:.4f}'.format(step, loss))\n",
    "    return w, trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Task 4: Implement the function to obtain the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def predict(X, w):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N_test, D]\n",
    "        (Augmented) feature matrix.\n",
    "    w : array, shape [D]\n",
    "        Regression coefficients (w[0] is the bias term).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : array, shape [N_test]\n",
    "        A binary array of predictions.\n",
    "    \"\"\"\n",
    "    return (sigmoid(np.dot(X, w)) > 0.5).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Full batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Change this to True if you want to see loss values over iterations.\n",
    "verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_train = X_train.shape[0]\n",
    "w_full, trace_full = logistic_regression(X_train, \n",
    "                                         y_train, \n",
    "                                         num_steps=8000, \n",
    "                                         learning_rate=1e-5, \n",
    "                                         mini_batch_size=n_train, \n",
    "                                         lmbda=0.1,\n",
    "                                         verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_train = X_train.shape[0]\n",
    "w_minibatch, trace_minibatch = logistic_regression(X_train, \n",
    "                                                   y_train, \n",
    "                                                   num_steps=8000, \n",
    "                                                   learning_rate=1e-5, \n",
    "                                                   mini_batch_size=50, \n",
    "                                                   lmbda=0.1,\n",
    "                                                   verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full batch: accuracy: 0.9240, f1_score: 0.9384\n",
      "Mini-batch: accuracy: 0.9415, f1_score: 0.9533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cp/c5k9r_5s289bkrd9pyvnf9cm0000gn/T/ipykernel_1438/2365100852.py:15: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return (sigmoid(np.dot(X, w)) > 0.5).astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "y_pred_full = predict(X_test, w_full)\n",
    "y_pred_minibatch = predict(X_test, w_minibatch)\n",
    "\n",
    "print('Full batch: accuracy: {:.4f}, f1_score: {:.4f}'\n",
    "      .format(accuracy_score(y_test, y_pred_full), f1_score(y_test, y_pred_full)))\n",
    "print('Mini-batch: accuracy: {:.4f}, f1_score: {:.4f}'\n",
    "      .format(accuracy_score(y_test, y_pred_minibatch), f1_score(y_test, y_pred_minibatch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "plt.plot(trace_full, label='Full batch')\n",
    "plt.plot(trace_minibatch, label='Mini-batch')\n",
    "plt.xlabel('Iterations * 50')\n",
    "plt.ylabel('Loss $\\mathcal{L}(\\mathbf{w})$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
